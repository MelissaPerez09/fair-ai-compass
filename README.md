# Fair AI with COMPAS Dataset

This project explores bias, fairness, and responsible AI using the **COMPAS Recidivism** dataset.  
The dataset has been widely studied for its potential racial and gender bias in predicting whether a person will reoffend.

## Objectives
- Perform exploratory data analysis (EDA) to detect imbalances.
- Identify potential biases in the dataset and in predictive models.
- Train a baseline model to predict recidivism.
- Evaluate fairness metrics across sensitive groups (race, gender, age).
- Apply at least one mitigation strategy to reduce bias.
- Compare model performance before and after mitigation.
- Reflect on ethical implications of deploying such models.

## Repository Structure
- `data/` → dataset or download instructions  
- `src/` → Jupyter Notebook(s) with the full analysis  
- `results/` → figures, metrics, and tables  
- `docs/` → GitHub Pages presentation  

## Getting Started
1. Clone this repository:
   ```bash
   git clone https://github.com/MelissaPerez09/fair-ai-compass
   cd fair-ai-compas
